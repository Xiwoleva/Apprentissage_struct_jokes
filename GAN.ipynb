{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import load\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "cuda = True\n",
    "taille_vecteur = 300\n",
    "hidden_dim = 20\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full(9708, 0) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/home/ash/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full(10549, 1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/home/ash/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full(10000, 2) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_j = load.load_jokes()\n",
    "X_b = load.load_bbc()\n",
    "X_r = load.load_random(10000)\n",
    "\n",
    "y_j = np.full(len(X_j),0).tolist()\n",
    "y_b = np.full(len(X_b),1).tolist()\n",
    "y_r = np.full(len(X_r),2).tolist()\n",
    "\n",
    "X = [] \n",
    "X += X_j\n",
    "X += X_b\n",
    "X += X_r\n",
    "\n",
    "y = [] \n",
    "y += y_j\n",
    "y += y_b\n",
    "y += y_r\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "idx = np.arange(len(X))\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# w2v = gensim.models.Word2Vec.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True) \n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
    "def to_vec(sentence):\n",
    "    res = np.empty((len(sentence) +1, taille_vecteur)) \n",
    "    for i in range(len(sentence)):\n",
    "        try:\n",
    "            res[i] = w2v.wv[sentence[i]]\n",
    "        except KeyError:\n",
    "            res[i] = w2v.wv[\"the\"]\n",
    "    res[-1] = np.zeros(taille_vecteur)\n",
    "    return res\n",
    "\n",
    "def doc_to_vec(X):\n",
    "    res = []\n",
    "    for i in range(len(X)):\n",
    "        res += [to_vec(X[i])]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = doc_to_vec(X)\n",
    "X_j = doc_to_vec(X_j)\n",
    "\n",
    "del(X_b)\n",
    "del(X_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifieur(nn.Module):\n",
    "\n",
    "    def __init__(self, taille_vecteur, hidden_dim, num_layers, cuda=False):\n",
    "        super(Classifieur, self).__init__()\n",
    "        self.taille_vecteur = taille_vecteur\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(taille_vecteur, hidden_dim, num_layers=num_layers)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        self.lin = nn.Linear(hidden_dim , int(hidden_dim/2))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(int(hidden_dim/2) , 3)\n",
    "        self.softmax = nn.Softmax(dim=1) \n",
    "        \n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        \n",
    "        \n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.cuda:\n",
    "            return (autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim).cuda()),\n",
    "                    autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim).cuda()))\n",
    "        else:\n",
    "            return (autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)),\n",
    "                    autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):    \n",
    "\n",
    "        out, self.hidden = self.lstm(inputs.view(-1,1,300), self.hidden)\n",
    "        out = out[-1].view(-1, self.hidden_dim)\n",
    "        out = self.lin(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "    def prep_label(self, y):\n",
    "        if self.cuda:\n",
    "            return autograd.Variable(torch.LongTensor(y).cuda())\n",
    "        else:\n",
    "            return autograd.Variable(torch.LongTensor(y))\n",
    "    \n",
    "    def train_epoch(self, X, y):\n",
    "\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters())\n",
    "        if self.cuda:\n",
    "            loss_function = nn.CrossEntropyLoss().cuda()\n",
    "        else:\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss_cum = 0\n",
    "        acc = 0\n",
    "\n",
    "        for j in range(len(X)):\n",
    "\n",
    "            self.zero_grad()\n",
    "            self.hidden = self.init_hidden()\n",
    "            vectorize = X[j]\n",
    "\n",
    "            if self.cuda:\n",
    "                tens_X = autograd.Variable(torch.FloatTensor(vectorize).cuda())\n",
    "            else:\n",
    "                tens_X = autograd.Variable(torch.FloatTensor(vectorize))\n",
    "\n",
    "\n",
    "            output = self.forward(tens_X)\n",
    "            loss = loss_function(output, self.prep_label(np.array(y[j])))\n",
    "\n",
    "            if self.cuda:\n",
    "                loss_cum += loss.data.cpu().numpy()\n",
    "                if np.argmax(output.data.cpu().numpy()) == y[j]:\n",
    "                    acc += 1\n",
    "            else:\n",
    "                loss_cum += loss.data.numpy()\n",
    "                if np.argmax(output.data.numpy()) == y[j]:\n",
    "                    acc += 1\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return acc/len(X), loss_cum/len(X)\n",
    "\n",
    "    def train(self, X, y, epoch=10):\n",
    "        for i in range(epoch):\n",
    "            print(\"epoch\", i)\n",
    "            acc , loss = self.train_epoch(X, y)\n",
    "            print(\"acc : {}, loss : {}\".format(acc, loss))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generateur(nn.Module):\n",
    "    \n",
    "    def __init__(self, taille_vecteur, hidden_dim, num_layers):\n",
    "        super(Generateur, self).__init__()\n",
    "        self.taille_vecteur = taille_vecteur\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "       \n",
    "        self.lstm = nn.LSTM(taille_vecteur, hidden_dim, num_layers=num_layers)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.lin = nn.Linear(hidden_dim , taille_vecteur)\n",
    "        \n",
    "\n",
    "    def init_hidden(self):\n",
    "        if cuda:\n",
    "            return (autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim).cuda().float()),\n",
    "                    autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim).cuda().float()))\n",
    "        else:\n",
    "            return (autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)),\n",
    "                    autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "#         print(inputs.view(-1,1,300))\n",
    "        out, self.hidden = self.lstm(inputs.view(1,1,-1).float(), self.hidden)\n",
    "        out = out[-1].view(-1, self.hidden_dim)\n",
    "        out = self.lin(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminent(nn.Module):\n",
    "    \n",
    "    def __init__(self, taille_vecteur, hidden_dim, num_layers):\n",
    "        super(Discriminent, self).__init__()\n",
    "        self.taille_vecteur = taille_vecteur\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(taille_vecteur, hidden_dim, num_layers=num_layers)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.lin = nn.Linear(hidden_dim , 1)\n",
    "        \n",
    "\n",
    "    def init_hidden(self):\n",
    "        if cuda:\n",
    "            return (autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim).cuda().float()),\n",
    "                    autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim).cuda().float()))\n",
    "        else:\n",
    "            return (autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)),\n",
    "                    autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        out, self.hidden = self.lstm(inputs.view(-1,1,300), self.hidden)\n",
    "        out = out[-1].view(-1, self.hidden_dim)\n",
    "\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return nn.Sigmoid()(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_stop(w):\n",
    "        \n",
    "        dist = np.linalg.norm(w)\n",
    "\n",
    "        if dist < 0.2:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "def gen(model, seeds):\n",
    "\n",
    "\n",
    "    all_blague = []\n",
    "    if cuda:\n",
    "        tens_s = autograd.Variable(torch.from_numpy(seeds).float().cuda())\n",
    "    else:\n",
    "        tens_s = autograd.Variable(torch.from_numpy(seeds).float())\n",
    "        \n",
    "    for j in range(len(seeds)):\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "#         print(tens_s[j])\n",
    "        output =[model(tens_s[j])]\n",
    "        if cuda:\n",
    "            res = [output[-1].data.cpu().numpy().reshape(300)]\n",
    "        else:\n",
    "            res = [output[-1].data.numpy().reshape(300)]\n",
    "            \n",
    "        for i in range(1, 30):\n",
    "            output += [model(output[i-1].view(300))]\n",
    "            if cuda:\n",
    "                res += [output[-1].data.cpu().numpy().reshape(300)]\n",
    "            else:\n",
    "                res += [output[-1].data.numpy().reshape(300)]\n",
    "\n",
    "            if cuda:\n",
    "                if is_stop(output[-1].data.cpu().numpy().reshape(300)):\n",
    "                    break\n",
    "            else:\n",
    "                if is_stop(output[-1].data.numpy().reshape(300)):\n",
    "                    break\n",
    "\n",
    "        all_blague += [res]\n",
    "\n",
    "    return all_blague"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit(generateur, discriminent, classifieur, X, y, X_j, epoch=5, d_epoch=10, g_epoch=1, batch=500):\n",
    "    print(\"classifieur train\")\n",
    "#     classifieur.train(X, y, epoch=2)\n",
    "    print(\"fin train classifieur\")\n",
    "    \n",
    "    if cuda:\n",
    "        loss_function = nn.BCELoss().cuda()\n",
    "        loss_function_oracle = nn.CrossEntropyLoss().cuda() \n",
    "    else:\n",
    "        loss_function = nn.BCELoss()\n",
    "        loss_function_oracle = nn.CrossEntropyLoss()\n",
    "        \n",
    "    opti_G = optim.SGD(generateur.parameters(), lr=0.001)\n",
    "    opti_D = optim.Adam(discriminent.parameters())\n",
    "    \n",
    "    loss_cum_d = np.empty(0)\n",
    "    loss_cum_g = np.empty(0)\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        print(\"epoch gan :\", e)\n",
    "\n",
    "        for d_e in range(0, d_epoch, ):\n",
    "            print(\"epoch d :\", d_e)\n",
    "            loss_cum = 0\n",
    "            \n",
    "            for i in  range(0, len(X_j), batch):\n",
    "                seeds = np.random.normal(0, 1, (batch, 300))\n",
    "\n",
    "                d_fake = gen(generateur, seeds)\n",
    "                d_real = X_j[batch*i: batch*(i+1)].copy()\n",
    "                \n",
    "                idx = np.arange(len(d_fake)+len(d_real))\n",
    "                np.random.shuffle(idx)\n",
    "\n",
    "                if cuda:\n",
    "#                     d_all = autograd.Variable(torch.FloatTensor(np.array(d_real+ d_fake)[idx]).cuda())\n",
    "                    y_all = autograd.Variable(torch.FloatTensor(np.concatenate((np.ones(len(d_real)), np.zeros(len(d_fake))))[idx]).cuda())\n",
    "                else:\n",
    "#                     d_all = autograd.Variable(torch.FloatTensor(np.array(d_real+ d_fake)[idx]))\n",
    "                    y_all = autograd.Variable(torch.FloatTensor(np.concatenate((np.ones(len(d_real)), np.zeros(len(d_fake))))[idx]))\n",
    "                d_all = np.array(d_real+ d_fake)[idx]\n",
    "#                 y_all =np.concatenate((np.ones(len(d_real)), np.zeros(len(d_fake))))[idx]\n",
    "    \n",
    "                del d_fake\n",
    "                del d_real\n",
    "                \n",
    "                discriminent.zero_grad()\n",
    "\n",
    "                for i in range(len(d_all)):\n",
    "                    discriminent.hidden = discriminent.init_hidden()\n",
    "                    if cuda:\n",
    "                        inp = autograd.Variable(torch.FloatTensor(np.array(d_all[i])).cuda())\n",
    "#                         target = autograd.Variable(torch.FloatTensor(y_all[i].reshape(1, 1)).float().cuda())\n",
    "                    else:\n",
    "                        inp = autograd.Variable(torch.FloatTensor(np.array(d_all[i])))\n",
    "#                         target = autograd.Variable(torch.FloatTensor(y_all[i].reshape(1, 1)).float())\n",
    "\n",
    "                    out = discriminent(inp)\n",
    "#                     print(out.data.cpu().numpy(), target)\n",
    "                    loss = loss_function(out, y_all[i])\n",
    "\n",
    "                    if cuda:\n",
    "                        loss_cum += loss.data.cpu().numpy()\n",
    "                    else:\n",
    "                        loss_cum += loss.data.numpy()\n",
    "\n",
    "                    if i%batch == 0:\n",
    "                        loss.backward()\n",
    "                        opti_D.step()\n",
    "                        discriminent.zero_grad()\n",
    "\n",
    "\n",
    "                loss.backward()\n",
    "                opti_D.step()\n",
    "            \n",
    "            loss_cum_d = np.concatenate((loss_cum_d, np.array(loss_cum)) )\n",
    "                \n",
    "\n",
    "            del d_all\n",
    "\n",
    "        for g_e in range(g_epoch):\n",
    "            print(\"epoch g :\", g_e)\n",
    "            loss_cum=0\n",
    "            for j in range(0, len(X_j), batch):\n",
    "                generateur.zero_grad()\n",
    "                generateur.hidden = generateur.init_hidden()\n",
    "                \n",
    "                seeds = np.random.normal(0, 1, (batch, 300))\n",
    "                g_fake = gen(generateur, seeds)\n",
    "            \n",
    "                for i in range(len(g_fake)):\n",
    "                    discriminent.hidden = discriminent.init_hidden()\n",
    "                    \n",
    "                    if cuda:\n",
    "                        inp = autograd.Variable(torch.FloatTensor(np.array(g_fake[i])).cuda())\n",
    "                    else:\n",
    "                        inp = autograd.Variable(torch.FloatTensor(np.array(g_fake[i])))\n",
    "                    out = discriminent(inp)\n",
    "\n",
    "                    if cuda:\n",
    "                        loss = loss_function(out, autograd.Variable(torch.from_numpy(np.ones(1))).cuda().float())\n",
    "                    else:\n",
    "                        loss = loss_function(out, autograd.Variable(torch.from_numpy(np.ones(1))).float())\n",
    "\n",
    "                    if cuda:\n",
    "                         loss_cum += loss.data.cpu().numpy()\n",
    "                    else:\n",
    "\n",
    "                        loss_cum += loss.data.numpy()\n",
    "\n",
    "                loss.backward()\n",
    "                opti_G.step()\n",
    "            loss_cum_g = np.concatenate((loss_cum_g, np.array(loss_cum)) )\n",
    "            \n",
    "        del g_fake\n",
    "        \n",
    "    return loss_cum_d, loss_cum_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifieur = Classifieur(taille_vecteur, hidden_dim, num_layers, cuda).cuda()\n",
    "generateur = Generateur(taille_vecteur, hidden_dim, num_layers).cuda()\n",
    "discriminent = Discriminent(taille_vecteur, hidden_dim, num_layers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifieur train\n",
      "fin train classifieur\n",
      "epoch gan : 0\n",
      "epoch g : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:26: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/ash/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1360: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch g : 1\n",
      "epoch gan : 1\n",
      "epoch g : 0\n",
      "epoch g : 1\n",
      "epoch gan : 2\n",
      "epoch g : 0\n",
      "epoch g : 1\n",
      "epoch gan : 3\n",
      "epoch g : 0\n",
      "epoch g : 1\n"
     ]
    }
   ],
   "source": [
    "loss_d, loss_g = fit(g, d, None, X, y, X_j, epoch=4, d_epoch=0, g_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4638.30371094  1273.52929688   780.14007568   592.40264893   475.76641846\n",
      "   392.38818359   311.57058716   256.90881348   217.20759583   177.59469604\n",
      "   153.33242798   124.49014282   109.13217926    93.04379272    82.70018005\n",
      "    71.48114777    61.78045273    56.10108948    51.69104004    49.18974686\n",
      "    44.51403427    38.18915558    34.07662964    30.67892647    28.01355553\n",
      "    26.0185833     23.48913956    22.04730988    20.44248581    19.55508804\n",
      "    18.07347679    16.62158775    15.74840736    14.76042843    13.85342216\n",
      "    12.91252613    12.31938362    11.66024399    11.14214993    10.56401062]\n"
     ]
    }
   ],
   "source": [
    "print(loss_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 49814.9375    60585.953125  67422.421875  72423.515625]\n"
     ]
    }
   ],
   "source": [
    "print(loss_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seeds = np.random.normal(0, 1, (15, 300))\n",
    "g_fake = gen(generateur, seeds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pickle.dump(g_fake, open(\"./g_fake.pkl\", mode='wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d = torch.load(\"discriminent.net\")\n",
    "g = torch.load(\"generateur.net\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g_fake = pickle.load(open(\"./g_fake.pkl\", mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(g_fake)):\n",
    "    for i in range(len(g_fake[j])):\n",
    "        print(w2v.most_similar([g_fake[j][i]])[0][0], end=\" \")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "94px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
